# trainer.py (Version 10.0 - Gradient Descent with Normalization)
import json
import math
import sys
from model import get_features, normalize_features, predict

if __name__ == "__main__":
    # --- 1. Load Data ---
    with open('public_cases.json', 'r') as f:
        data = json.load(f)

    # --- 2. Feature Analysis: Calculate Scaling Parameters ---
    print("Step 1: Analyzing data to determine feature scaling parameters...")

    # Get a list of all feature names
    feature_names = get_features(1, 1, 1).keys()
    scaling_params = {name: {'min': float('inf'), 'max': float('-inf')} for name in feature_names}

    for case in data:
        inputs = case['input']
        features = get_features(inputs['trip_duration_days'],
                                inputs['miles_traveled'],
                                inputs['total_receipts_amount'])
        for name, value in features.items():
            if value < scaling_params[name]['min']:
                scaling_params[name]['min'] = value
            if value > scaling_params[name]['max']:
                scaling_params[name]['max'] = value

    print("Feature scaling parameters calculated.")

    # --- 3. Prepare Data for Training ---
    # Now, process all data into normalized feature sets
    training_set = []
    for case in data:
        inputs = case['input']
        raw_features = get_features(inputs['trip_duration_days'],
                                    inputs['miles_traveled'],
                                    inputs['total_receipts_amount'])
        normalized_features = normalize_features(raw_features, scaling_params)
        training_set.append({
            'features': normalized_features,
            'expected': case['expected_output']
        })

    # --- 4. Initialize Model and Hyperparameters ---
    weights = {name: 0.0 for name in feature_names}
    LEARNING_RATE = 0.05  # We can use a MUCH larger learning rate now
    NUM_EPOCHS = 20000

    print(f"\nStep 2: Starting training with Learning Rate: {LEARNING_RATE}")

    # --- 5. The Training Loop ---
    for epoch in range(NUM_EPOCHS):
        total_error = 0.0
        gradients = {name: 0.0 for name in weights.keys()}

        for item in training_set:
            features = item['features']
            expected = item['expected']

            prediction = predict(features, weights)
            error = prediction - expected
            total_error += error ** 2

            for name, value in features.items():
                gradients[name] += error * value

        num_cases = len(training_set)
        for name in weights.keys():
            avg_gradient = gradients[name] / num_cases
            weights[name] -= LEARNING_RATE * avg_gradient

        if (epoch + 1) % 1000 == 0:
            rmse = math.sqrt(total_error / num_cases)
            print(f"Epoch {epoch+1}/{NUM_EPOCHS} | RMSE: {rmse:.4f}")

    print("\nüèÅ Training Complete.")

    # --- 6. Generate the Final Submission Script ---
    with open('model.py', 'r') as f:
        model_code = f.read()

    main_block = f"""
if __name__ == "__main__":
    import sys
    # --- OPTIMIZED WEIGHTS AND SCALING PARAMS (Generated by trainer.py) ---
    weights = {json.dumps(weights, indent=4)}
    scaling_params = {json.dumps(scaling_params, indent=4)}

    if len(sys.argv) != 4:
        sys.exit(1)
    try:
        duration = int(sys.argv[1])
        miles = float(sys.argv[2])
        receipts = float(sys.argv[3])

        # The final script must perform the same feature engineering AND normalization
        raw_features = get_features(duration, miles, receipts)
        normalized_features = normalize_features(raw_features, scaling_params)

        result = predict(normalized_features, weights)
        print(f"{{result:.2f}}")
    except Exception:
        print(0.0)
        sys.exit(0)
"""
    final_code = model_code + main_block
    with open('solution.py', 'w') as f:
        f.write(final_code)

    print(f"‚úÖ Generated final 'solution.py' with learned weights and scaling.")